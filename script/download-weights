import os
import torch

# 设置以下环境变量
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
torch.backends.cuda.matmul.allow_tf32 = False
torch.backends.cudnn.benchmark = False
torch.backends.cudnn.deterministic = True

# 导入其他库
import hashlib
import datetime
import numpy as np
from cog import BasePredictor, Input, Path
import ChatTTS
import time

# Constants
MODEL_DIR = "models"
MODEL_PATH = os.path.join(MODEL_DIR, 'asset/spk_stat.pt')

# Check if model files exist
if not os.path.exists(MODEL_PATH):
    raise FileNotFoundError(f"Model files not found at {MODEL_DIR}. Please ensure the model files are present.")

chat = ChatTTS.Chat()
chat.load_models(source="local", local_path=MODEL_DIR)

class Predictor(BasePredictor):
    def setup(self):
        """Load the model into memory to make running multiple predictions efficient"""
        self.std, self.mean = torch.load(MODEL_PATH).chunk(2)
    
    @torch.inference_mode()
    def predict(self,
                text: str = Input(description="Text to be synthesized", default="Hello world!"),
                voice: int = Input(description="Voice identifier", default=2222),
                custom_voice: int = Input(description="Custom voice identifier", default=0),
                skip_refine: int = Input(description="Skip refine text step", default=0),
                temperature: float = Input(description="Temperature for sampling", default=0.3),
                top_p: float = Input(description="Top-p sampling parameter", default=0.7),
                top_k: int = Input(description="Top-k sampling parameter", default=20),
                prompt: str = Input(description="Prompt for refining text", default=''),
    ) -> dict:
        if custom_voice > 0:
            voice = custom_voice
        
        torch.manual_seed(voice)
        rand_spk = chat.sample_random_speaker()
        
        # Calculate the file hash
        md5_hash = hashlib.md5()
        md5_hash.update(f"{text}-{voice}-{skip_refine}-{prompt}".encode('utf-8'))
        datename = datetime.datetime.now().strftime('%Y%m%d-%H_%M_%S')
        filename = datename + '-' + md5_hash.hexdigest() + ".wav"
        
        # Inference
        start_time = time.time()
        
        wavs = chat.infer(
            [t for t in text.split("\n") if t.strip()],
            use_decoder=True,
            skip_refine_text=True if int(skip_refine) == 1 else False,
            params_infer_code={'spk_emb': rand_spk, 'temperature': temperature, 'top_P': top_p, 'top_K': top_k},
            params_refine_text={'prompt': prompt},
            do_text_normalization=False
        )
        
        end_time = time.time()
        inference_time = round(end_time - start_time, 2)
        
        combined_wavdata = np.concatenate([wav[0] for wav in wavs])
        
        sample_rate = 24000  # Assuming 24kHz sample rate
        audio_duration = round(len(combined_wavdata) / sample_rate, 2)
        
        # Save the resulting WAV file
        output_path = Path("/tmp") / filename
        # sf.write(str(output_path), combined_wavdata, sample_rate)
        
        # Create response dict
        response = {
            "code": 0,
            "msg": "ok",
            "audio_files": [{
                "filename": filename,
                "audio_data": output_path,
                "inference_time": inference_time,
                "audio_duration": audio_duration
            }]
        }
        
        return response